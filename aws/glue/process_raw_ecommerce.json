{
	"jobConfig": {
		"name": "process_raw_ecommerce",
		"description": "",
		"role": "arn:aws:iam::017742597587:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "process_raw_ecommerce.py",
		"scriptLocation": "s3://aws-glue-assets-017742597587-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-03-28T02:29:45.032Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-017742597587-us-east-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-017742597587-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"sourceControlDetails": {
			"Provider": "GITHUB",
			"Repository": "",
			"Branch": ""
		},
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys, uuid\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.context import DynamicFrameCollection\nfrom awsglue.transforms import Join\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DateType, BooleanType\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import Row\n\n# data preprocessing\ndef data_preprocessing(df): \n    # Perform preprocessing, drop null and convert data types. \n    print(\"Data Preprocessing...\")\n    # Drop rows with null Order_Number\n    df = df.filter(df[\"Order_Number\"].isNotNull())\n    # Convert column names to lowercase\n    for col in df.columns:\n        df = df.withColumnRenamed(col, col.lower())\n    # Convert order_date to proper date type\n    # In PySpark, we use to_date function instead of pandas to_datetime\n    df = df.withColumn(\"order_date\", \n                      F.to_date(F.col(\"order_date\"), \"dd/MM/yyyy\"))\n    # Convert back to DynamicFrame and return\n    return df\n\n# process date dimension\ndef proc_date_dim(df, glueContext, spark):\n    # create and process date dimension\n    print(\"Processing date dimension...\")\n    # Extract unique dates - need to ensure order_date column exists and is a date type\n    if \"order_date\" not in df.columns:\n        raise ValueError(\"order_date column not found in the input data\")\n    # Select distinct dates and create a dataframe with just the dates\n    dates_df = df.select(\"order_date\").distinct()\n    dates_df = dates_df.withColumnRenamed(\"order_date\", \"date_full\")\n    # Add date attributes - similar to pandas operations but using Spark functions\n    dates_df = dates_df.withColumn(\"day_of_week\", F.dayofweek(\"date_full\"))\n    dates_df = dates_df.withColumn(\"day_name\", F.date_format(\"date_full\", \"EEEE\"))\n    dates_df = dates_df.withColumn(\"day_of_month\", F.dayofmonth(\"date_full\"))\n    dates_df = dates_df.withColumn(\"day_of_year\", F.dayofyear(\"date_full\"))\n    dates_df = dates_df.withColumn(\"week_of_year\", F.weekofyear(\"date_full\"))\n    dates_df = dates_df.withColumn(\"month_num\", F.month(\"date_full\"))\n    dates_df = dates_df.withColumn(\"month_name\", F.date_format(\"date_full\", \"MMMM\"))\n    dates_df = dates_df.withColumn(\"quarter\", F.quarter(\"date_full\"))\n    dates_df = dates_df.withColumn(\"year\", F.year(\"date_full\"))\n    # Check if day_of_week is Saturday (7) or Sunday (1)\n    # In Spark SQL, dayofweek returns 1-7 where 1=Sunday, 7=Saturday\n    dates_df = dates_df.withColumn(\n        \"is_weekend\", \n        (F.col(\"day_of_week\") == 1) | (F.col(\"day_of_week\") == 7)\n    )\n    # Convert back to DynamicFrame and return\n    return DynamicFrame.fromDF(dates_df, glueContext, \"date_dimension\")\n\n# process customer dimension \ndef proc_cust_dim(df, glueContext):\n    print(\"Processing customer dimension...\")\n    # Extract unique customer names\n    customers_df = df.select(\"customer_name\").distinct()\n    # Create surrogate keys using row_number() window function\n    window_spec = Window.orderBy(\"customer_name\")\n    customers_df = customers_df.withColumn(\"customer_id\", \n                                          F.row_number().over(window_spec))\n    # Split customer name into first and last name\n    customers_df = customers_df.withColumn(\"first_name\", \n                                          F.split(F.col(\"customer_name\"), \" \").getItem(0))\n    customers_df = customers_df.withColumn(\"last_name\", \n                                          F.expr(\"split(customer_name, ' ')[size(split(customer_name, ' '))-1]\"))\n    # Add metadata columns with current timestamp\n    current_timestamp = F.current_timestamp()\n    customers_df = customers_df.withColumn(\"create_date\", current_timestamp)\n    customers_df = customers_df.withColumn(\"update_date\", current_timestamp)\n    # Convert back to DynamicFrame and return\n    return DynamicFrame.fromDF(customers_df, glueContext, \"customer_dimension\")\n\n# process geo dimensions \ndef generate_india_states_list(spark):\n    # Create list of Indian states and union territories with their details\n    india_states = [\n        {\"state_code\": \"AN\", \"state_name\": \"Andaman and Nicobar Islands\", \n         \"capital_city\": \"Port Blair\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-AN\"},\n        {\"state_code\": \"AP\", \"state_name\": \"Andhra Pradesh\", \n         \"capital_city\": \"Amaravati\", \"status\": \"State\", \n         \"iso_code\": \"IN-AP\"},\n        {\"state_code\": \"AR\", \"state_name\": \"Arunachal Pradesh\", \n         \"capital_city\": \"Itanagar\", \"status\": \"State\", \"iso_code\": \"IN-AR\"},\n        {\"state_code\": \"AS\", \"state_name\": \"Assam\", \"capital_city\": \"Dispur\", \n         \"status\": \"State\", \"iso_code\": \"IN-AS\"},\n        {\"state_code\": \"BR\", \"state_name\": \"Bihar\", \"capital_city\": \"Patna\", \n         \"status\": \"State\", \"iso_code\": \"IN-BR\"},\n        {\"state_code\": \"CH\", \"state_name\": \"Chandigarh\", \n         \"capital_city\": \"Chandigarh\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-CH\"},\n        {\"state_code\": \"CG\", \"state_name\": \"Chhattisgarh\", \n         \"capital_city\": \"Raipur\", \"status\": \"State\", \"iso_code\": \"IN-CG\"},\n        {\"state_code\": \"DD\", \n         \"state_name\": \"Daman and Diu\", \n         \"capital_city\": \"Daman\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-DD\"},\n        {\"state_code\": \"DH\", \n         \"state_name\": \"Dadra and Nagar Haveli and Daman and Diu\", \n         \"capital_city\": \"Daman\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-DH\"},\n        {\"state_code\": \"DL\", \"state_name\": \"Delhi\", \n         \"capital_city\": \"New Delhi\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-DL\"},\n        {\"state_code\": \"GA\", \"state_name\": \"Goa\", \"capital_city\": \"Panaji\", \n         \"status\": \"State\", \"iso_code\": \"IN-GA\"},\n        {\"state_code\": \"GJ\", \"state_name\": \"Gujarat\", \n         \"capital_city\": \"Gandhinagar\", \"status\": \"State\", \"iso_code\": \"IN-GJ\"},\n        {\"state_code\": \"HR\", \"state_name\": \"Haryana\", \n         \"capital_city\": \"Chandigarh\", \"status\": \"State\", \"iso_code\": \"IN-HR\"},\n        {\"state_code\": \"HP\", \"state_name\": \"Himachal Pradesh\", \n         \"capital_city\": \"Shimla\", \"status\": \"State\", \"iso_code\": \"IN-HP\"},\n        {\"state_code\": \"JK\", \"state_name\": \"Jammu and Kashmir\", \n         \"capital_city\": \"Srinagar/Jammu\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-JK\"},\n        {\"state_code\": \"JH\", \"state_name\": \"Jharkhand\", \n         \"capital_city\": \"Ranchi\", \"status\": \"State\", \"iso_code\": \"IN-JH\"},\n        {\"state_code\": \"KA\", \"state_name\": \"Karnataka\", \n         \"capital_city\": \"Bengaluru\", \"status\": \"State\", \"iso_code\": \"IN-KA\"},\n        {\"state_code\": \"KL\", \"state_name\": \"Kerala\", \n         \"capital_city\": \"Thiruvananthapuram\", \"status\": \"State\", \n         \"iso_code\": \"IN-KL\"},\n        {\"state_code\": \"LA\", \"state_name\": \"Ladakh\", \n         \"capital_city\": \"Leh\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-LA\"},\n        {\"state_code\": \"LD\", \"state_name\": \"Lakshadweep\", \n         \"capital_city\": \"Kavaratti\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-LD\"},\n        {\"state_code\": \"MP\", \"state_name\": \"Madhya Pradesh\", \n         \"capital_city\": \"Bhopal\", \"status\": \"State\", \"iso_code\": \"IN-MP\"},\n        {\"state_code\": \"MH\", \"state_name\": \"Maharashtra\", \n         \"capital_city\": \"Mumbai\", \"status\": \"State\", \"iso_code\": \"IN-MH\"},\n        {\"state_code\": \"MN\", \"state_name\": \"Manipur\", \n         \"capital_city\": \"Imphal\", \"status\": \"State\", \"iso_code\": \"IN-MN\"},\n        {\"state_code\": \"ML\", \"state_name\": \"Meghalaya\", \n         \"capital_city\": \"Shillong\", \"status\": \"State\", \"iso_code\": \"IN-ML\"},\n        {\"state_code\": \"MZ\", \"state_name\": \"Mizoram\", \n         \"capital_city\": \"Aizawl\", \"status\": \"State\", \"iso_code\": \"IN-MZ\"},\n        {\"state_code\": \"NL\", \"state_name\": \"Nagaland\", \n         \"capital_city\": \"Kohima\", \"status\": \"State\", \"iso_code\": \"IN-NL\"},\n        {\"state_code\": \"OD\", \"state_name\": \"Odisha\", \n         \"capital_city\": \"Bhubaneswar\", \"status\": \"State\", \n         \"iso_code\": \"IN-OD\"},\n        {\"state_code\": \"OR\", \"state_name\": \"Orissa\", \n         \"capital_city\": \"Bhubaneswar\", \"status\": \"State\", \n         \"iso_code\": \"IN-OR\"},\n        {\"state_code\": \"PY\", \"state_name\": \"Puducherry\", \n         \"capital_city\": \"Puducherry\", \"status\": \"Union Territory\", \n         \"iso_code\": \"IN-PY\"},\n        {\"state_code\": \"PB\", \"state_name\": \"Punjab\", \n         \"capital_city\": \"Chandigarh\", \"status\": \"State\", \n         \"iso_code\": \"IN-PB\"},\n        {\"state_code\": \"RJ\", \"state_name\": \"Rajasthan\", \n         \"capital_city\": \"Jaipur\", \"status\": \"State\", \"iso_code\": \"IN-RJ\"},\n        {\"state_code\": \"SK\", \"state_name\": \"Sikkim\", \n         \"capital_city\": \"Gangtok\", \"status\": \"State\", \"iso_code\": \"IN-SK\"},\n        {\"state_code\": \"TN\", \"state_name\": \"Tamil Nadu\", \n         \"capital_city\": \"Chennai\", \"status\": \"State\", \"iso_code\": \"IN-TN\"},\n        {\"state_code\": \"TG\", \"state_name\": \"Telangana\", \n         \"capital_city\": \"Hyderabad\", \"status\": \"State\", \"iso_code\": \"IN-TS\"},\n        {\"state_code\": \"TR\", \"state_name\": \"Tripura\", \n         \"capital_city\": \"Agartala\", \"status\": \"State\", \"iso_code\": \"IN-TR\"},\n        {\"state_code\": \"UP\", \"state_name\": \"Uttar Pradesh\", \n         \"capital_city\": \"Lucknow\", \"status\": \"State\", \"iso_code\": \"IN-UP\"},\n        {\"state_code\": \"UK\", \"state_name\": \"Uttarakhand\", \n         \"capital_city\": \"Dehradun\", \"status\": \"State\", \"iso_code\": \"IN-UK\"},\n        {\"state_code\": \"WB\", \"state_name\": \"West Bengal\", \n         \"capital_city\": \"Kolkata\", \"status\": \"State\", \"iso_code\": \"IN-WB\"}\n    ]\n    # Add UUID for each state and territory and country\n    for state in india_states:\n        state['state_id'] = str(uuid.uuid4())\n        state['country'] = 'India'\n    # Convert to dataframe\n    rows = [Row(**state) for state in india_states] \n    india_state_df = spark.createDataFrame(rows)\n    # Reorder columns to make ID first\n    columns_order = ['state_id', 'state_code', 'country', 'state_name', \n                     'capital_city', 'status', 'iso_code']\n    india_state_df = india_state_df.select(columns_order)\n    # Convert to DynamicFrame and return\n    return india_state_df\n\n# generate a list of U.S. states\ndef generate_us_states_list(spark):\n    # Create list of U.S. states and territories with their details\n    us_states = [\n        {\"state_code\": \"AL\", \"state_name\": \"Alabama\", \n         \"capital_city\": \"Montgomery\", \"status\": \"State\", \"iso_code\": \"US-AL\"},\n        {\"state_code\": \"AK\", \"state_name\": \"Alaska\", \n         \"capital_city\": \"Juneau\", \"status\": \"State\", \"iso_code\": \"US-AK\"},\n        {\"state_code\": \"AZ\", \"state_name\": \"Arizona\", \n         \"capital_city\": \"Phoenix\", \"status\": \"State\", \"iso_code\": \"US-AZ\"},\n        {\"state_code\": \"AR\", \"state_name\": \"Arkansas\", \n         \"capital_city\": \"Little Rock\", \"status\": \"State\", \n         \"iso_code\": \"US-AR\"},\n        {\"state_code\": \"CA\", \"state_name\": \"California\", \n         \"capital_city\": \"Sacramento\", \"status\": \"State\", \n         \"iso_code\": \"US-CA\"},\n        {\"state_code\": \"CO\", \"state_name\": \"Colorado\", \n         \"capital_city\": \"Denver\", \"status\": \"State\", \"iso_code\": \"US-CO\"},\n        {\"state_code\": \"CT\", \"state_name\": \"Connecticut\", \n         \"capital_city\": \"Hartford\", \"status\": \"State\", \"iso_code\": \"US-CT\"},\n        {\"state_code\": \"DE\", \"state_name\": \"Delaware\", \n         \"capital_city\": \"Dover\", \"status\": \"State\", \"iso_code\": \"US-DE\"},\n        {\"state_code\": \"FL\", \"state_name\": \"Florida\", \n         \"capital_city\": \"Tallahassee\", \"status\": \"State\", \n         \"iso_code\": \"US-FL\"},\n        {\"state_code\": \"GA\", \"state_name\": \"Georgia\", \n         \"capital_city\": \"Atlanta\", \"status\": \"State\", \"iso_code\": \"US-GA\"},\n        {\"state_code\": \"HI\", \"state_name\": \"Hawaii\", \n         \"capital_city\": \"Honolulu\", \"status\": \"State\", \"iso_code\": \"US-HI\"},\n        {\"state_code\": \"ID\", \"state_name\": \"Idaho\", \n         \"capital_city\": \"Boise\", \"status\": \"State\", \"iso_code\": \"US-ID\"},\n        {\"state_code\": \"IL\", \"state_name\": \"Illinois\", \n         \"capital_city\": \"Springfield\", \"status\": \"State\", \n         \"iso_code\": \"US-IL\"},\n        {\"state_code\": \"IN\", \"state_name\": \"Indiana\", \n         \"capital_city\": \"Indianapolis\", \"status\": \"State\", \n         \"iso_code\": \"US-IN\"},\n        {\"state_code\": \"IA\", \"state_name\": \"Iowa\", \n         \"capital_city\": \"Des Moines\", \"status\": \"State\", \"iso_code\": \"US-IA\"},\n        {\"state_code\": \"KS\", \"state_name\": \"Kansas\", \n         \"capital_city\": \"Topeka\", \"status\": \"State\", \"iso_code\": \"US-KS\"},\n        {\"state_code\": \"KY\", \"state_name\": \"Kentucky\", \n         \"capital_city\": \"Frankfort\", \"status\": \"State\", \"iso_code\": \"US-KY\"},\n        {\"state_code\": \"LA\", \"state_name\": \"Louisiana\", \n         \"capital_city\": \"Baton Rouge\", \"status\": \"State\", \n         \"iso_code\": \"US-LA\"},\n        {\"state_code\": \"ME\", \"state_name\": \"Maine\", \n         \"capital_city\": \"Augusta\", \"status\": \"State\", \"iso_code\": \"US-ME\"},\n        {\"state_code\": \"MD\", \"state_name\": \"Maryland\", \n         \"capital_city\": \"Annapolis\", \"status\": \"State\", \"iso_code\": \"US-MD\"},\n        {\"state_code\": \"MA\", \"state_name\": \"Massachusetts\", \n         \"capital_city\": \"Boston\", \"status\": \"State\", \"iso_code\": \"US-MA\"},\n        {\"state_code\": \"MI\", \"state_name\": \"Michigan\", \n         \"capital_city\": \"Lansing\", \"status\": \"State\", \"iso_code\": \"US-MI\"},\n        {\"state_code\": \"MN\", \"state_name\": \"Minnesota\", \n         \"capital_city\": \"St. Paul\", \"status\": \"State\", \"iso_code\": \"US-MN\"},\n        {\"state_code\": \"MS\", \"state_name\": \"Mississippi\", \n         \"capital_city\": \"Jackson\", \"status\": \"State\", \"iso_code\": \"US-MS\"},\n        {\"state_code\": \"MO\", \"state_name\": \"Missouri\", \n         \"capital_city\": \"Jefferson City\", \"status\": \"State\", \n         \"iso_code\": \"US-MO\"},\n        {\"state_code\": \"MT\", \"state_name\": \"Montana\", \n         \"capital_city\": \"Helena\", \"status\": \"State\", \"iso_code\": \"US-MT\"},\n        {\"state_code\": \"NE\", \"state_name\": \"Nebraska\", \n         \"capital_city\": \"Lincoln\", \"status\": \"State\", \"iso_code\": \"US-NE\"},\n        {\"state_code\": \"NV\", \"state_name\": \"Nevada\", \n         \"capital_city\": \"Carson City\", \"status\": \"State\", \n         \"iso_code\": \"US-NV\"},\n        {\"state_code\": \"NH\", \"state_name\": \"New Hampshire\", \n         \"capital_city\": \"Concord\", \"status\": \"State\", \"iso_code\": \"US-NH\"},\n        {\"state_code\": \"NJ\", \"state_name\": \"New Jersey\", \n         \"capital_city\": \"Trenton\", \"status\": \"State\", \"iso_code\": \"US-NJ\"},\n        {\"state_code\": \"NM\", \"state_name\": \"New Mexico\", \n         \"capital_city\": \"Santa Fe\", \"status\": \"State\", \"iso_code\": \"US-NM\"},\n        {\"state_code\": \"NY\", \"state_name\": \"New York\", \n         \"capital_city\": \"Albany\", \"status\": \"State\", \"iso_code\": \"US-NY\"},\n        {\"state_code\": \"NC\", \"state_name\": \"North Carolina\", \n         \"capital_city\": \"Raleigh\", \"status\": \"State\", \"iso_code\": \"US-NC\"},\n        {\"state_code\": \"ND\", \"state_name\": \"North Dakota\", \n         \"capital_city\": \"Bismarck\", \"status\": \"State\", \"iso_code\": \"US-ND\"},\n        {\"state_code\": \"OH\", \"state_name\": \"Ohio\", \n         \"capital_city\": \"Columbus\", \"status\": \"State\", \"iso_code\": \"US-OH\"},\n        {\"state_code\": \"OK\", \"state_name\": \"Oklahoma\", \n         \"capital_city\": \"Oklahoma City\", \"status\": \"State\", \n         \"iso_code\": \"US-OK\"},\n        {\"state_code\": \"OR\", \"state_name\": \"Oregon\", \n         \"capital_city\": \"Salem\", \"status\": \"State\", \"iso_code\": \"US-OR\"},\n        {\"state_code\": \"PA\", \"state_name\": \"Pennsylvania\", \n         \"capital_city\": \"Harrisburg\", \"status\": \"State\", \"iso_code\": \"US-PA\"},\n        {\"state_code\": \"RI\", \"state_name\": \"Rhode Island\", \n         \"capital_city\": \"Providence\", \"status\": \"State\", \"iso_code\": \"US-RI\"},\n        {\"state_code\": \"SC\", \"state_name\": \"South Carolina\", \n         \"capital_city\": \"Columbia\", \"status\": \"State\", \"iso_code\": \"US-SC\"},\n        {\"state_code\": \"SD\", \"state_name\": \"South Dakota\", \n         \"capital_city\": \"Pierre\", \"status\": \"State\", \"iso_code\": \"US-SD\"},\n        {\"state_code\": \"TN\", \"state_name\": \"Tennessee\", \n         \"capital_city\": \"Nashville\", \"status\": \"State\", \"iso_code\": \"US-TN\"},\n        {\"state_code\": \"TX\", \"state_name\": \"Texas\", \n         \"capital_city\": \"Austin\", \"status\": \"State\", \"iso_code\": \"US-TX\"},\n        {\"state_code\": \"UT\", \"state_name\": \"Utah\", \n         \"capital_city\": \"Salt Lake City\", \"status\": \"State\", \n         \"iso_code\": \"US-UT\"},\n        {\"state_code\": \"VT\", \"state_name\": \"Vermont\", \n         \"capital_city\": \"Montpelier\", \"status\": \"State\", \"iso_code\": \"US-VT\"},\n        {\"state_code\": \"VA\", \"state_name\": \"Virginia\", \n         \"capital_city\": \"Richmond\", \"status\": \"State\", \"iso_code\": \"US-VA\"},\n        {\"state_code\": \"WA\", \"state_name\": \"Washington\", \n         \"capital_city\": \"Olympia\", \"status\": \"State\", \"iso_code\": \"US-WA\"},\n        {\"state_code\": \"WV\", \"state_name\": \"West Virginia\", \n         \"capital_city\": \"Charleston\", \"status\": \"State\", \"iso_code\": \"US-WV\"},\n        {\"state_code\": \"WI\", \"state_name\": \"Wisconsin\", \n         \"capital_city\": \"Madison\", \"status\": \"State\", \"iso_code\": \"US-WI\"},\n        {\"state_code\": \"WY\", \"state_name\": \"Wyoming\", \n         \"capital_city\": \"Cheyenne\", \"status\": \"State\", \"iso_code\": \"US-WY\"},\n        {\"state_code\": \"DC\", \"state_name\": \"District of Columbia\", \n         \"capital_city\": \"Washington\", \"status\": \"Federal District\", \n         \"iso_code\": \"US-DC\"},\n        {\"state_code\": \"AS\", \"state_name\": \"American Samoa\", \n         \"capital_city\": \"Pago Pago\", \"status\": \"Territory\", \n         \"iso_code\": \"US-AS\"},\n        {\"state_code\": \"GU\", \"state_name\": \"Guam\", \n         \"capital_city\": \"Hagåtña\", \"status\": \"Territory\", \n         \"iso_code\": \"US-GU\"},\n        {\"state_code\": \"MP\", \"state_name\": \"Northern Mariana Islands\", \n         \"capital_city\": \"Saipan\", \"status\": \"Territory\", \"iso_code\": \"US-MP\"},\n        {\"state_code\": \"PR\", \"state_name\": \"Puerto Rico\", \n         \"capital_city\": \"San Juan\", \"status\": \"Territory\", \n         \"iso_code\": \"US-PR\"},\n        {\"state_code\": \"VI\", \"state_name\": \"U.S. Virgin Islands\", \n         \"capital_city\": \"Charlotte Amalie\", \"status\": \"Territory\", \n         \"iso_code\": \"US-VI\"}\n    ]\n    # Add UUID for each state/territory\n    for state in us_states:\n        state['state_id'] = str(uuid.uuid4())\n        state['country'] = 'United States'\n    # Convert to dataframe\n    rows = [Row(**state) for state in us_states]\n    us_state_df = spark.createDataFrame(rows) \n    # Reorder columns to make ID first\n    columns_order = ['state_id', 'state_code', 'country', 'state_name', \n                     'capital_city', 'status', 'iso_code']\n    us_state_df = us_state_df.select(columns_order)\n    return us_state_df\n\n# generate a list of canadian provinces \ndef generate_canadian_provinces_list(spark):\n    # Create list of province/territory data\n    canada_provinces = [\n        {\"state_code\": \"AB\", \"state_name\": \"Alberta\", \n         \"capital_city\": \"Edmonton\", \"status\": \"Province\",\n         \"iso_code\": \"CA-AB\"},\n        {\"state_code\": \"BC\", \"state_name\": \"British Columbia\", \n         \"capital_city\": \"Victoria\", \"status\": \"Province\",\n         \"iso_code\": \"CA-BC\"},\n        {\"state_code\": \"MB\", \"state_name\": \"Manitoba\", \n         \"capital_city\": \"Winnipeg\", \"status\": \"Province\", \n         \"iso_code\": \"CA-MB\"},\n        {\"state_code\": \"NB\", \"state_name\": \"New Brunswick\", \n         \"capital_city\": \"Fredericton\", \"status\": \"Province\", \n         \"iso_code\": \"CA-NB\"},\n        {\"state_code\": \"NL\", \"state_name\": \"Newfoundland and Labrador\", \n         \"capital_city\": \"St. John's\", \"status\": \"Province\", \n         \"iso_code\": \"CA-NL\"},\n        {\"state_code\": \"NS\", \"state_name\": \"Nova Scotia\", \n         \"capital_city\": \"Halifax\", \"status\": \"Province\", \n         \"iso_code\": \"CA-NS\"},\n        {\"state_code\": \"ON\", \"state_name\": \"Ontario\", \n         \"capital_city\": \"Toronto\", \"status\": \"Province\", \n         \"iso_code\": \"CA-ON\"},\n        {\"state_code\": \"PE\", \"state_name\": \"Prince Edward Island\", \n         \"capital_city\": \"Charlottetown\", \"status\": \"Province\", \n         \"iso_code\": \"CA-PE\"},\n        {\"state_code\": \"QC\", \"state_name\": \"Quebec\", \n         \"capital_city\": \"Quebec City\", \"status\": \"Province\",\n         \"iso_code\": \"CA-QC\"},\n        {\"state_code\": \"SK\", \"state_name\": \"Saskatchewan\", \n         \"capital_city\": \"Regina\", \"status\": \"Province\", \n         \"iso_code\": \"CA-SK\"},\n        {\"state_code\": \"NT\", \"state_name\": \"Northwest Territories\", \n         \"capital_city\": \"Yellowknife\", \"status\": \"Territory\",\n         \"iso_code\": \"CA-NT\"},\n        {\"state_code\": \"NU\", \"state_name\": \"Nunavut\", \n         \"capital_city\": \"Iqaluit\", \"status\": \"Territory\",\n         \"iso_code\": \"CA-NU\"},\n        {\"state_code\": \"YT\", \"state_name\": \"Yukon\", \n         \"capital_city\": \"Whitehorse\", \"status\": \"Territory\",\n         \"iso_code\": \"CA-YT\"}\n    ]\n    # Add UUID for each province/territory and country\n    for province in canada_provinces:\n        province['state_id'] = str(uuid.uuid4())\n        province['country'] = 'Canada'\n    # Convert to dataframe\n    rows = [Row(**province) for province in canada_provinces]\n    canada_provinces_df = spark.createDataFrame(rows)\n    # Reorder columns to make ID first\n    columns_order = ['state_id', 'state_code', 'country', 'state_name', \n                     'capital_city', 'status', 'iso_code']\n    canada_provinces_df = canada_provinces_df.select(columns_order)\n    # Convert to DynamicFrame and return\n    return canada_provinces_df\n\n# based on the list generated above\n# obtain a comprehensive list of states in U.S., Canada, and India.\ndef proc_geo_dim(spark, glueContext): \n    print('Processing geography dimension...')\n    # Generate states/provinces for India, U.S., and Canada\n    geographys_india = generate_india_states_list(spark)\n    geographys_us = generate_us_states_list(spark)\n    geographys_canada = generate_canadian_provinces_list(spark)\n    # Concatenate DataFrames\n    geographys = geographys_india.union(geographys_us).union(geographys_canada)\n    # add metadata\n    geographys = geographys.withColumn(\"created_at\", F.current_timestamp())\n    # Cache the result to improve performance\n    geographys_df = geographys.cache()\n    return DynamicFrame.fromDF(geographys_df, glueContext, \"geography_dimension\")\n\n# process product dimension\ndef proc_prod_dim(df, glueContext):\n    print('Processing product dimension...')\n    # Extract unique product information\n    products = df.select('product', 'category', 'brand').distinct()\n    # Add surrogate key using row_number window function\n    window_spec = Window.orderBy('product', 'category', 'brand')\n    products = products.withColumn('product_id', F.row_number().over(window_spec))\n    # Get unique product-cost combinations\n    pc = df.select('product', 'category', 'brand', 'cost').distinct()\n    # Ensure there is only one cost associated with each product\n    # Count unique costs per product\n    pc_counts = pc.groupBy('product', 'category', 'brand').agg(\n        F.countDistinct('cost').alias('costcnt')\n    )\n    # Filter to keep only products with a single cost value\n    pc_valid = pc_counts.filter(F.col('costcnt') == 1)\n    # Join back to get the cost value\n    pc = pc.join(pc_valid, on=['product', 'category', 'brand'], how='inner')\n    pc = pc.drop('costcnt')\n    # Merge cost information with product table\n    products = products.join(\n        pc.select('product', 'category', 'brand', 'cost'),\n        on=['product', 'category', 'brand'],\n        how='left'\n    )\n    # Rename columns\n    products = products.withColumnRenamed('product', 'product_name')\n    products = products.withColumnRenamed('cost', 'standard_cost')\n    # Fill null values with 0 for standard_cost\n    products = products.na.fill({'standard_cost': 0})\n    # Add metadata columns\n    current_timestamp = F.current_timestamp()\n    products = products.withColumn('create_date', current_timestamp)\n    products = products.withColumn('update_date', current_timestamp)\n    return DynamicFrame.fromDF(products, glueContext, \"product_dimension\")\n\n\ndef proc_ostatus_dim(df, glueContext): \n    print('Processing order status dimension...')\n    # Extract unique statuses\n    status = df.select('status').distinct()\n    # Add surrogate key \n    window_spec = Window.orderBy('status')\n    status = status.withColumn('status_id', F.row_number().over(window_spec))\n    # Rename columns\n    status = status.withColumnRenamed('status', 'status_name')\n    \n    # Instead of using a UDF, use when/otherwise for mapping\n    status = status.withColumn(\n        'status_description',\n        F.when(F.col('status_name') == 'Delivered', 'Order has been delivered')\n         .when(F.col('status_name') == 'Order', 'Order has been placed')\n         .when(F.col('status_name') == 'Processing', 'Order is being processed')\n         .when(F.col('status_name') == 'Shipped', 'Order has been shipped')\n         .otherwise('Unknown status')\n    )\n    \n    # Add metadata columns\n    current_timestamp = F.current_timestamp()\n    status = status.withColumn('create_date', current_timestamp)\n    status = status.withColumn('update_date', current_timestamp)\n    return DynamicFrame.fromDF(status, glueContext, \"status_dimension\")\n\n# process employee/supervisor dimension\ndef proc_emp_dim(df, glueContext): \n    print('Processing employee/supervisor dimension...')\n    # Extract unique supervisor names\n    employee = df.select('assigned supervisor').distinct()\n    # Add surrogate key\n    window_spec = Window.orderBy('assigned supervisor')\n    employee = employee.withColumn('employee_id', F.row_number().over(window_spec))\n    # Rename columns\n    employee = employee.withColumnRenamed('assigned supervisor', 'employee_name')\n    # Split names to get first and last names\n    employee = employee.withColumn('employee_first_name', \n                                 F.split(F.col('employee_name'), ' ').getItem(0))\n    employee = employee.withColumn('employee_last_name',\n                                 F.expr(\"split(employee_name, ' ')[size(split(employee_name, ' '))-1]\"))\n    # Add metadata columns\n    current_timestamp = F.current_timestamp()\n    employee = employee.withColumn('create_date', current_timestamp)\n    employee = employee.withColumn('update_date', current_timestamp)\n    # Convert back to DynamicFrame and return\n    return DynamicFrame.fromDF(employee, glueContext, \"employee_dimension\")\n\n\ndef fact_table(df, dates, customers, geographys, products, status, employee, glueContext, spark): \n    print(\"Creating fact table...\")\n    # First check what we're working with\n    # Use the DataFrame directly without trying to convert it\n    print(\"Using orders DataFrame directly\")\n    orders_df = df  # Already a DataFrame, no conversion needed\n    # Only convert the dimension DynamicFrames to DataFrames\n    print(\"Converting dimension DynamicFrames to DataFrames\")\n    customers_df = customers.toDF()\n    geo_df = geographys.toDF()\n    products_df = products.toDF()\n    status_df = status.toDF()\n    employee_df = employee.toDF()\n\n    # Pre-filter and select only needed columns from geo dimension\n    print(\"Filtering geography to India\")\n    geo_india_df = geo_df.filter(F.col(\"country\") == \"India\").select(\"state_code\", \"state_id\")\n    \n    # Prepare all dimension DataFrames with only the columns needed for joining\n    print(\"Preparing dimension tables for joins\")\n    customers_join_df = customers_df.select(\"customer_name\", \"customer_id\")\n    products_join_df = products_df.select(F.col(\"product_name\").alias(\"product\"), \"product_id\")\n    status_join_df = status_df.select(F.col(\"status_name\").alias(\"status\"), \"status_id\")\n    employee_join_df = employee_df.select(F.col(\"employee_name\").alias(\"assigned supervisor\"), \"employee_id\")\n    \n    # Perform joins - use broadcast for smaller dimension tables\n    print(\"Joining fact table with dimensions\")\n    orders_df = orders_df.join(F.broadcast(customers_join_df), on=\"customer_name\", how=\"left\")\n    orders_df = orders_df.join(F.broadcast(geo_india_df), on=\"state_code\", how=\"left\")\n    orders_df = orders_df.join(F.broadcast(products_join_df), on=\"product\", how=\"left\")\n    orders_df = orders_df.join(F.broadcast(status_join_df), on=\"status\", how=\"left\")\n    orders_df = orders_df.join(F.broadcast(employee_join_df), on=\"assigned supervisor\", how=\"left\")\n    \n    # Rename columns\n    orders_df = orders_df.withColumnRenamed(\"cost\", \"unit_cost\") \\\n                         .withColumnRenamed(\"sales\", \"unit_sales\")\n    \n    # Calculate derived columns in one step when possible\n    print(\"Calculating derived columns\")\n    orders_df = orders_df.withColumn(\"profit\", F.col(\"total_sales\") - F.col(\"total_cost\"))\n    orders_df = orders_df.withColumn(\n        \"profit_margin\", \n        F.when(F.col(\"total_sales\") > 0, \n              (F.col(\"total_sales\") - F.col(\"total_cost\")) / F.col(\"total_sales\")\n        ).otherwise(F.lit(0))\n    )\n    \n    # Select required columns\n    required_columns = [\n        'order_number', 'order_date', 'customer_id', 'state_id', \n        'product_id', 'status_id', 'employee_id', 'unit_cost', \n        'unit_sales', 'quantity', 'total_cost', 'total_sales', \n        'profit', 'profit_margin'\n    ]\n    orders_df = orders_df.select(required_columns)\n    \n    # Handle missing values in one operation\n    print(\"Handling missing values\")\n    null_defaults = {\n        \"customer_id\": -1, \"state_id\": -1, \"product_id\": -1,\n        \"status_id\": -1, \"employee_id\": -1, \"unit_cost\": 0,\n        \"unit_sales\": 0, \"total_cost\": 0, \"total_sales\": 0,\n        \"profit\": 0, \"profit_margin\": 0\n    }\n    orders_df = orders_df.na.fill(null_defaults)\n    \n    # Convert to DynamicFrame after all DataFrame operations are complete\n    print(\"Converting to DynamicFrame\")\n    return DynamicFrame.fromDF(orders_df, glueContext, \"orders_fact_table\")\n\ndef save_dfs_to_s3(glueContext, dataframes, file_names, \nbucket_name, folder_path, format=\"parquet\"):\n    print('Saving Data to S3...')\n    # Verify inputs are valid\n    if len(dataframes) != len(file_names):\n        raise ValueError(f\"\"\"Number of dataframes ({len(dataframes)}) \\\n        must match number of file names ({len(file_names)})\"\"\")\n    # Loop through dataframes and save each one\n    for i, (df, file_name) in enumerate(zip(dataframes, file_names)):\n        print(f\"Saving {file_name} to S3 ({i+1}/{len(dataframes)})...\")\n        # Create S3 path\n        s3_path = f\"s3://{bucket_name}/{folder_path}{file_name}\"\n        \n        # Convert to regular DataFrame and write using Spark's write method\n        spark_df = df.toDF()\n        spark_df.write.mode(\"overwrite\").parquet(s3_path)\n        \n        print(f\"Successfully saved {file_name} to {s3_path}\")\n    print(f\"\"\"All {len(dataframes)} dataframes saved successfully \\\n    to s3://{bucket_name}/{folder_path}\"\"\")\n\ndef main(): \n    print(\"Starting ETL job...\")\n    # Initialize Glue job\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    job = Job(glueContext)\n    job.init(args['JOB_NAME'], args)\n    \n    # customize the environment\n    source_database = \"raw_ecommerce_db\"\n    source_table = \"online_ecommerce_csv\"\n    target_bucket = \"aws-bucket-ecommerce\"\n    target_folder = \"processed/\"\n    \n    print(f\"Reading data from {source_database}.{source_table}...\")\n    # Read data from catalog\n    try:\n        raw_order = glueContext.create_dynamic_frame.from_catalog(\n            database=source_database,\n            table_name=source_table,\n            transformation_ctx=\"source_data\"\n        )\n        \n        # convert to dataframe\n        raw_order_df = raw_order.toDF()\n        print(f\"Successfully read data: {raw_order_df.count()} rows\")\n        \n        # preprocessing \n        df = data_preprocessing(raw_order_df)\n        print(f\"Preprocessed data: {df.count()} rows\")\n        print(f\"Columns: {df.columns}\")\n        \n        # transform and create dimensions\n        dim_date = proc_date_dim(df, glueContext, spark)\n        dim_cust = proc_cust_dim(df, glueContext)\n        dim_geo = proc_geo_dim(spark, glueContext)\n        dim_prod = proc_prod_dim(df, glueContext)\n        dim_ostatus = proc_ostatus_dim(df, glueContext)\n        dim_emp = proc_emp_dim(df, glueContext)\n        \n        # transform fact table, orders\n        fact_orders = fact_table(df, dim_date, dim_cust, dim_geo, \n             dim_prod, dim_ostatus, dim_emp, glueContext, spark)\n        \n        # Prepare for saving\n        files = [dim_date, dim_cust, dim_geo, dim_prod, dim_ostatus, \n                 dim_emp, fact_orders]\n        file_names = ['dim_date', 'dim_cust', 'dim_geo', 'dim_prod', \n                      'dim_ostatus', 'dim_emp', 'fact_orders']\n        # Save all dataframes to S3\n        save_dfs_to_s3(\n            glueContext=glueContext,  # This is now correctly passed\n            dataframes=files,         # Changed from dfs1 to dataframes\n            file_names=file_names,\n            bucket_name=target_bucket,\n            folder_path=target_folder,\n            format=\"parquet\"\n        )\n        \n    except Exception as e:\n        print(f\"Error in ETL process: {str(e)}\")\n        raise\n        \n    job.commit()\n    print(\"ETL job completed successfully\")\n\n# Add this to the end of your script to call the main function\nif __name__ == \"__main__\":\n    main()"
}